# IRIS RAG System Configuration
# This file contains all configurable constants used throughout the application.
# Modify these values to adjust system behavior without changing code.

# Document processing configuration
document_processing:
  # Traditional chunking settings
  default_chunk_size: 500      # Words per chunk when splitting documents
  default_chunk_overlap: 50    # Word overlap between consecutive chunks
  
  # Sectional chunking settings
  enable_sectional_chunking: true     # Use structure-aware chunking for DOD documents
  max_section_chunk_size: 800         # Maximum words per section-based chunk
  section_split_threshold: 1200       # Force split sections larger than this
  preserve_subsection_integrity: true # Keep subsections (1.1, 1.2) together when possible
  
  # Hierarchical sectional chunking (single optimal size for all tiers)
  sectional_chunking:
    target_chunk_size: 700     # Optimal size for semantic coherence across all LLM tiers
    min_chunk_size: 200        # Don't split sections smaller than this
    max_chunk_size: 1200       # Force split if larger than this
    enable_hierarchical_grouping: true  # Group subsections under parent sections
  
  # Performance optimization settings
  enable_section_caching: true  # Cache section detection results
  max_cache_entries: 100       # Maximum number of cached section results
  
  # Layout analysis constants
  header_footer_zone_ratio: 0.1  # Fraction of page height for header/footer zones (0.1 = 10%)
  line_tolerance_pixels: 5       # Pixel tolerance for grouping text on same line
  
  # Text filtering thresholds
  min_header_footer_chars: 50    # Minimum characters to not be header/footer
  max_header_footer_words: 3     # Maximum words for header/footer detection
  min_line_length: 15           # Minimum line length to include in cleaned text
  first_page_sample_chars: 2000  # Characters to extract as first page sample

# Web interface configuration
web_interface:
  default_port: 8080           # Port for Flask web interface
  default_host: "localhost"    # Host for Flask web interface

# Timeout configurations (in seconds)
timeouts:
  default_query_timeout: 60    # Query processing timeout
  default_load_docs_timeout: 300  # Document loading timeout (5 minutes)

# ChromaDB configuration
chromadb:
  default_persist_dir: "database"        # Directory for ChromaDB persistence
  default_collection_name: "iris_documents"  # Default collection name
  similarity_metric: "cosine"            # Distance metric for similarity search

# Document-aware ranking configuration
ranking:
  # Boost multipliers for document similarity (applied to similarity scores)
  same_document_boost: 1.5              # Boost for chunks from exact same filename
  same_doc_number_boost: 1.4            # Boost for exact document number match
  same_specific_range_boost: 1.35       # Boost for same specific range (first 3 digits)
  same_subgroup_boost: 1.3              # Boost for same subgroup (second digit * 100)
  same_major_group_boost: 1.2           # Boost for same major group (first digit * 1000)
  
  # Adjacency boost multipliers (multiplicative with document boosts, same filename only)
  adjacent_chunk_boost: 1.3             # Boost for adjacent chunks (±1 chunk distance)
  near_chunk_boost: 1.15                # Boost for near chunks (±2 chunk distance)
  nearby_chunk_boost: 1.05              # Boost for nearby chunks (±3-5 chunk distance)
  
  # Ranking strategy
  enable_document_aware_ranking: true    # Enable enhanced document-aware ranking
  apply_boosts_to_top_result: true      # Apply boosts even to the top-ranked result

# LLM configuration
llm:
  default_context_window: 4096          # Default context window size for all models
  fallback_max_tokens: 256              # Default max tokens when model config not found
  fallback_temperature: 0.7             # Default temperature when model config not found
  fallback_top_p: 0.95                  # Default top_p when model config not found
  
  # Model-specific configurations for Ollama models
  # Organized by performance tiers (0=minimum, 4=maximum)
  models:
    # Tier 0: Minimum spec (2-4GB RAM)
    "tinyllama:1.1b-chat-v1-q4_K_M":
      tier: 0
      max_tokens: 256
      temperature: 0.7
      top_p: 0.9
      prompt_style: "simple"
      context_window: 2048
    
    # Tier 1: Low spec (4-6GB RAM)  
    "llama3.2:1b-instruct-q4_K_M":
      tier: 1
      max_tokens: 400
      temperature: 0.7
      top_p: 0.9
      prompt_style: "instruct"
      context_window: 4096
    
    # Tier 2: Standard spec (6-8GB RAM)
    "llama3.2:3b-instruct-q4_K_M":
      tier: 2
      max_tokens: 512
      temperature: 0.5
      top_p: 0.95
      prompt_style: "instruct"
      context_window: 4096
    
    # Tier 3: High spec (8-12GB RAM)
    "gemma2:9b-instruct-q4_K_M":
      tier: 3
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.9
      prompt_style: "instruct"
      context_window: 8192
    
    # Tier 4: Premium spec (12GB+ RAM)
    "phi4-mini:latest":
      tier: 4
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.9
      prompt_style: "instruct"
      context_window: 8192

# LLM Prompt Templates
prompts:
  simple:
    template: |
      Context from DOD policies:
      {context}
      
      Question: {question}
      
      Instructions: Answer based ONLY on the context above. If the context doesn't contain enough information to answer the question, say "The provided policies do not contain sufficient information to answer this question."
      
      Answer:
  
  instruct:
    template: |
      ### System:
      You are an expert assistant for Department of Defense policy questions. You must answer based ONLY on the provided context. Do not use any external knowledge or make assumptions beyond what is explicitly stated in the context.
      
      ### Context:
      {context}
      
      ### Question: {question}
      
      ### Instructions:
      - Answer using ONLY information found in the context above
      - If the context doesn't contain enough information, respond with "The provided policies do not contain sufficient information to answer this question"
      - Quote relevant parts of the policies when possible
      - Do not add information not present in the context
      
      ### Answer:
  
  mistral:
    template: |
      [INST] You are a helpful assistant that answers questions about Department of Defense policies based ONLY on provided excerpts.
      
      IMPORTANT RULES:
      - Use ONLY the information provided in the context below
      - Do not use external knowledge or make assumptions
      - If the context doesn't contain enough information, say "The provided policies do not contain sufficient information to answer this question"
      - Quote specific policy sections when relevant
      
      Context from DOD policies:
      {context}
      
      Question: {question} [/INST]
  
  fallback:
    template: |
      Context from DOD policies:
      {context}
      
      Question: {question}
      
      Instructions: Answer based ONLY on the context above. If the context doesn't contain enough information, say "The provided policies do not contain sufficient information to answer this question."
      
      Answer: